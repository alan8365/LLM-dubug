{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T05:11:55.707420Z",
     "start_time": "2024-02-05T05:11:55.665966Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m testdata \u001b[38;5;241m=\u001b[39m load_json_testcases(test_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m      7\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m ob \u001b[38;5;241m=\u001b[39m \u001b[43mtest_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtestdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m dt \u001b[38;5;241m=\u001b[39m testdata[i][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(ob, dt)\n",
      "File \u001b[0;32m~/PycharmProjects/debug/generated_code/gpt4/kheapsort/kheapsort_1.py:11\u001b[0m, in \u001b[0;36mkheapsort\u001b[0;34m(arr, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# For the remaining elements, if the heap is of size k,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# replace the smallest element with the current element if it is larger\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k, \u001b[38;5;28mlen\u001b[39m(arr)):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr[i] \u001b[38;5;241m>\u001b[39m \u001b[43mheap\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     12\u001b[0m         heapq\u001b[38;5;241m.\u001b[39mheappushpop(heap, arr[i])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract the sorted elements from the heap\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets.QuixBugs.python_testcases.load_testdata import load_json_testcases\n",
    "from generated_code.gpt4.kheapsort.kheapsort_1 import kheapsort as test_fn\n",
    "\n",
    "testdata = load_json_testcases(test_fn.__name__)\n",
    "\n",
    "i = 0\n",
    "ob = test_fn(*testdata[i][0])\n",
    "dt = testdata[i][1]\n",
    "\n",
    "print(ob, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76dd1129f060b7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-05T05:11:55.708500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240cddbcc8353b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T05:11:55.713984Z",
     "start_time": "2024-02-05T05:11:55.710583Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=open_ai_key, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Who won the world series in 2020.\"),\n",
    "    AIMessage(content=\"The Los Angeles Dodgers won the World Series in 2020.\"),\n",
    "    HumanMessage(content=\"How much time did the team won the World Series in 1980 to 2020?\"),\n",
    "]\n",
    "\n",
    "chat(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a32b7d7e0953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T05:16:03.010450Z",
     "start_time": "2024-02-05T05:15:58.184031Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided code is syntactically incorrect. It is missing `print` function's arguments enclosed in parentheses. The correct syntax for using the `print` function in Python is:\n",
      "\n",
      "```python\n",
      "print(*args, sep='', file=None, flush=False)\n",
      "```\n",
      "\n",
      "where:\n",
      "\n",
      "* `*args`: An arbitrary number of objects to be printed.\n",
      "* `sep`: (Optional) The separator to use when joining the objects' representations.\n",
      "* `file`: (Optional) The file where the data will be written. If not specified, `stdout` (standard output) will be used.\n",
      "* `flush`: (Optional) If `True`, the output will be flushed and written to the file immediately. If not specified, it defaults to `False`, meaning that output will be written to a temporary buffer and only written to the file once the `print` statement is fully executed.\n",
      "\n",
      "To fix the given code, you need to add parentheses:\n",
      "\n",
      "```python\n",
      "print('AAA')\n",
      "```\n",
      "\n",
      "This code will print the string `\"AAA\"` followed by a newline to the console.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "google_ai_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=google_ai_key)\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-pro\",\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "convo = model.start_chat()\n",
    "\n",
    "convo.send_message('''Fix the bugs in the following code:ï»¿\n",
    "```python=\n",
    "print('AAA)\n",
    "```''')\n",
    "print(convo.last.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38510008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "a = glob('./datasets/QuixBugs/python_programs/*.py')\n",
    "b = [Path(i).stem for i in a if not i.endswith('test.py') and not i.endswith('node.py')]\n",
    "b = sorted(b)\n",
    "\n",
    "len(b)\n",
    "b\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert the list b to JSON and save it to a file named 'output.json'\n",
    "with open('prog_names.json', 'w') as file:\n",
    "    json.dump(b, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text=\"I apologize, but I do not have access to real-time weather information. As a language model AI, I don't have the ability to retrieve current weather data for your specific location. To get the most accurate and up-to-date weather information for today, I recommend checking your local weather forecast through reliable sources such as weather websites, apps, or news outlets. These sources can provide you with detailed information about temperature, precipitation, wind speed, and other relevant weather conditions for your area.\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=os.getenv('CLAUDE_API_KEY'),\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the weather today?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9e163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I apologize, but I do not have access to real-time weather information. As a language model AI, I don't have the ability to retrieve current weather data for your specific location. To get the most accurate and up-to-date weather information for today, I recommend checking your local weather forecast through reliable sources such as weather websites, apps, or news outlets. These sources can provide you with detailed information about temperature, precipitation, wind speed, and other relevant weather conditions for your area.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8642d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquixbugs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuixBugsDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prompt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClaudeApi\n",
      "File \u001b[0;32m~/PycharmProjects/debug/src/quixbugs.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANG, CODE_TYPE, LIB\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src_types'"
     ]
    }
   ],
   "source": [
    "from src.quixbugs import QuixBugsDataset\n",
    "from src.prompt import Prompt\n",
    "\n",
    "from src.api import ClaudeApi\n",
    "\n",
    "dataset = QuixBugsDataset(\"python\")\n",
    "prompt = Prompt(dataset[0])\n",
    "\n",
    "# model_name: MODEL_NAME = \"gpt-3.5-turbo-0125\"\n",
    "# a = OpenAiApi(model_name)\n",
    "\n",
    "# model_name: MODEL_NAME = \"gemini-1.0-pro\"\n",
    "# a = GeminiApi(model_name)\n",
    "\n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "a = ClaudeApi(model_name)\n",
    "\n",
    "b = a.api_call(prompt)\n",
    "\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1b397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb55fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gpt4-python-basic', 'gpt4-python-with_lib', 'gpt35-python-with_lib_v2', '.DS_Store', 'claude-python-basic', 'gemini-python-with_lib_v2', 'gpt4-python-with_lib_v2', 'gemini-python-with_lib', 'gpt35-python-basic', 'gpt35-python-with_lib', 'gemini-python-basic'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "exp_result = {}\n",
    "base_dir = Path(str(os.getenv(\"EXPS_DIR\")))\n",
    "\n",
    "exp_names = base_dir.glob(\"*\")\n",
    "exp_names = [i.name for i in exp_names if \"java\" not in str(i)]\n",
    "\n",
    "for exp_name in exp_names:\n",
    "    exp_dir = base_dir / exp_name\n",
    "    eval_dir = exp_dir / \"evals\"\n",
    "\n",
    "    all_eval_path = list(eval_dir.glob('*'))\n",
    "    all_eval_path = sorted(all_eval_path, key = lambda p: int(p.name.split('.')[0]))\n",
    "\n",
    "    exp_col = {}\n",
    "\n",
    "    for eval_path in all_eval_path:\n",
    "        with open(eval_path, 'r') as f:\n",
    "            a = json.load(f)\n",
    "\n",
    "        prog_name = a[\"sample\"][\"prog_name\"]\n",
    "        testcase_num = a[\"sample\"][\"testcase_num\"]\n",
    "        patchs_eval = a[\"patchs_eval\"]\n",
    "\n",
    "        pass_ratio = sum([i[\"pass_num\"] for i in patchs_eval]) / (testcase_num * 3)\n",
    "\n",
    "        exp_col[prog_name] = pass_ratio\n",
    "\n",
    "    exp_result[exp_name] = exp_col\n",
    "\n",
    "exp_result.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94558689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(exp_result)\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "df.to_csv('temp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['c'], ['b'], ['b', 'c'], ['a'], ['a', 'c'], ['a', 'b'], ['a', 'b', 'c']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a' , 'b', 'c'] \n",
    "\n",
    "def powerset(arr):\n",
    "    if arr:\n",
    "        first, *rest = arr #python3 just like car and cdr (in this case anyway..)\n",
    "        rest_subsets = powerset(rest)\n",
    "        return rest_subsets + [[first] + subset for subset in rest_subsets]\n",
    "    else:\n",
    "        return [[]]\n",
    "    \n",
    "powerset(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        value=None,\n",
    "        successor=None,\n",
    "        successors=[],\n",
    "        predecessors=[],\n",
    "        incoming_nodes=[],\n",
    "        outgoing_nodes=[],\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.successor = successor\n",
    "        self.successors = successors\n",
    "        self.predecessors = predecessors\n",
    "        self.incoming_nodes = incoming_nodes\n",
    "        self.outgoing_nodes = outgoing_nodes\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Node: {self.value}\"\n",
    "\n",
    "\n",
    "node1 = Node(\"1\")\n",
    "node5 = Node(\"5\")\n",
    "node4 = Node(\"4\", None, [node5])\n",
    "node3 = Node(\"3\", None, [node4])\n",
    "node2 = Node(\"2\", None, [node1, node3, node4])\n",
    "node0 = Node(\"0\", None, [node2, node5])\n",
    "\n",
    "length_by_edge = {\n",
    "    (node0, node2): 3,\n",
    "    (node0, node5): 10,\n",
    "    (node2, node1): 1,\n",
    "    (node2, node3): 2,\n",
    "    (node2, node4): 4,\n",
    "    (node3, node4): 1,\n",
    "    (node4, node5): 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, Node: 0)]\n",
      "[(3, Node: 2), (10, Node: 5)]\n",
      "[(4, Node: 1), (7, Node: 4), (5, Node: 3), (10, Node: 5)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get(node_heap, wanted_node):\n",
    "    for dist, node in node_heap:\n",
    "        if node == wanted_node:\n",
    "            return dist\n",
    "    return 0\n",
    "\n",
    "\n",
    "def insert_or_update(node_heap, dist_node):\n",
    "    dist, node = dist_node\n",
    "    for i, tpl in enumerate(node_heap):\n",
    "        a, b = tpl\n",
    "        if b == node:\n",
    "            node_heap[i] = dist_node  # heapq retains sorted property\n",
    "            return None\n",
    "\n",
    "    heappush(node_heap, dist_node)\n",
    "    return None\n",
    "\n",
    "def shortest_path_length(length_by_edge, startnode, goalnode):\n",
    "    unvisited_nodes = []  # FibHeap containing (node, distance) pairs\n",
    "    heappush(unvisited_nodes, (0, startnode))\n",
    "    visited_nodes = set()\n",
    "\n",
    "    while len(unvisited_nodes) > 0:\n",
    "        print(unvisited_nodes)\n",
    "        distance, node = heappop(unvisited_nodes)\n",
    "        if node is goalnode:\n",
    "            return distance\n",
    "\n",
    "        visited_nodes.add(node)\n",
    "\n",
    "        for nextnode in node.successors:\n",
    "            if nextnode in visited_nodes:\n",
    "                continue\n",
    "\n",
    "            insert_or_update(\n",
    "                unvisited_nodes,\n",
    "                (\n",
    "                    min(\n",
    "                        get(unvisited_nodes, nextnode) or float(\"inf\"),\n",
    "                        get(unvisited_nodes, nextnode) + length_by_edge[node, nextnode],\n",
    "                    ),\n",
    "                    nextnode,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    return float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "result = shortest_path_length(length_by_edge, node0, node1)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35836ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def bucketsort(arr, k):\n",
      "    counts = [0] * k\n",
      "    for x in arr:\n",
      "        counts[x] += 1\n",
      "\n",
      "    sorted_arr = []\n",
      "    for i, count in enumerate(counts):  \n",
      "        sorted_arr.extend([i] * count)\n",
      "\n",
      "    return sorted_arr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tokenize\n",
    "import io\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Removes comments and docstrings from a Python source file.\n",
    "    \"\"\"\n",
    "    io_obj = io.StringIO(source)\n",
    "    out = \"\"\n",
    "    prev_toktype = tokenize.INDENT\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    for tok in tokenize.generate_tokens(io_obj.readline):\n",
    "        token_type = tok[0]\n",
    "        token_string = tok[1]\n",
    "        start_line, start_col = tok[2]\n",
    "        end_line, end_col = tok[3]\n",
    "        ltext = tok[4]\n",
    "        # The following two conditions preserve indentation\n",
    "        if start_line > last_lineno:\n",
    "            last_col = 0\n",
    "        if start_col > last_col:\n",
    "            out += (\" \" * (start_col - last_col))\n",
    "        # Remove comments:\n",
    "        if token_type == tokenize.COMMENT:\n",
    "            pass\n",
    "        else:\n",
    "            out += token_string\n",
    "        prev_toktype = token_type\n",
    "        last_col = end_col\n",
    "        last_lineno = end_line\n",
    "    return out\n",
    "\n",
    "source_code = \"\"\"\n",
    "def bucketsort(arr, k):\n",
    "    counts = [0] * k\n",
    "    for x in arr:\n",
    "        counts[x] += 1\n",
    "\n",
    "    sorted_arr = []\n",
    "    for i, count in enumerate(counts):  # enumerate(counts) not enumerate(arr)\n",
    "        sorted_arr.extend([i] * count)\n",
    "\n",
    "    return sorted_arr\n",
    "\"\"\"\n",
    "clean_code = remove_comments_and_docstrings(source_code)\n",
    "print(clean_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e9cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def git_diff_like(text1, text2):\n",
    "    text1_lines = text1.splitlines()\n",
    "    text2_lines = text2.splitlines()\n",
    "    \n",
    "    diff = difflib.unified_diff(\n",
    "        text1_lines, text2_lines, \n",
    "        fromfile='file1', tofile='file2', \n",
    "        lineterm='', n=3)\n",
    "    \n",
    "    return '\\n'.join(list(diff))\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"\"\"def bucketsort(arr, k):\n",
    "    counts = [0] * k\n",
    "    for x in arr:\n",
    "        counts[x] += 1\n",
    "\n",
    "    sorted_arr = []\n",
    "    for i, count in enumerate(counts):  \n",
    "        sorted_arr.extend([i] * count)\n",
    "\n",
    "    return sorted_arr\"\"\"\n",
    "modified_text = \"\"\"def bucketsort(arr, k):\n",
    "    counts = [0] * k\n",
    "    for x in arr:\n",
    "        counts[x] += 1\n",
    "\n",
    "    sorted_arr = []\n",
    "    for i, count in enumerate(counts):\n",
    "        sorted_arr.extend([i] * count)\n",
    "\n",
    "    return sorted_arr\"\"\"\n",
    "\n",
    "diff_output = git_diff_like(original_text, original_text)\n",
    "print(diff_output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
